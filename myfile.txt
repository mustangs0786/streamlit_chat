 Gonna do something crazy Gonna do something random It's gotta be stochastic It's gotta be Statquest Hello, I'm Josh Starmer and welcome to Statquest Today we're gonna talk about stochastic gradient descent and it's gonna be clearly explained Note this stat quest assumes that you are already familiar with gradient descent if not Check out the quest the link is in the description below This video picks up where the original leaves off providing more details about how stochastic gradient descent works and some of its more subtle advantages Now even though I just said you need to watch the stat quest on gradient descent Let's do a little review to demonstrate the problem that stochastic gradient descent solves In the stat quest on gradient descent we took this simple data set height and weight measurements from three different people and we wanted to fit a line to it using gradient descent However, at first we started out with this generic equation for a line and The goal was to find the optimal values for the intercept and the slope For example if we started with the intercept equals zero and the slope equals one Then we could use weight to predict height Then we would use the sum of the squared residuals as the loss function to determine how well the initial line fit the data Note the sum of the squared residuals is just one of many different loss functions that can evaluate how well something fits the data In this case that something is a line To find the optimal values for the intercept and slope We plugged the equation for the predicted height into the sum of the squared residuals Then we took the derivative of the sum of the squared residuals with respect to the intercept and With respect to the slope Then we plugged in the values from the observed data into the derivative with respect to the intercept and Then we did the same thing for the derivative with respect to the slope Then we plugged in the initial guess for the intercept zero and The initial guess for the slope one We did the math plug the slopes into the step size formulas and Multiplyed by the learning rate which we set to 0.01 Then we did the math Calculated the new intercept and new slope by plugging in the old intercept and old slope and the step sizes and We did the math and We ended up with a new intercept and a new slope Then we went back to the derivatives and repeated the process a lot of times until we took the maximum number of steps or the steps became very very small In this super simple example we were just fitting a line with two parameters the intercept and the slope and We only had three data points So we only had three terms to compute each step for the intercept and We only had three terms to compute each step for the slope So each step didn't require much math But what if we had a more complicated model like a logistic regression that used 23,000 genes to predict if someone will have a disease Then we will have 23,000 derivatives to plug the data into and What if we had data from one million samples? Then we would have to calculate one million terms for each of the 23,000 derivatives In other words, we'd have to calculate 23,000 terms for each step and Since it's common to take at least 1,000 steps we would calculate at least 2.3 trillion terms So for big data gradient descent is slow This is where stochastic gradient descent comes in handy Going back to our super simple example Stochastic gradient descent would randomly pick one sample for each step and Just use that one sample to calculate the derivatives Thus in this super simple example stochastic gradient descent reduced the number of terms computed by a factor of 3 If we had one million samples then stochastic gradient descent would reduce the amount of terms computed by a factor of 1 million So that's pretty cool Stochastic gradient descent is especially useful when there are redundancies in the data For example, we have 12 data points, but there is a lot of redundancy that forms three clusters So we start with a line with the intercept equals zero and the slope equals one Then we randomly pick this point So we plug in the weight three and height three point three Do the math plug in the slopes Then multiply by the learning rate Note just like with regular gradient descent stochastic gradient descent is sensitive to the value you choose for the learning rate and Just like for regular gradient descent the general strategy is to start with a relatively large learning rate and make it smaller with each step And lastly just like for regular gradient descent many implementations of stochastic gradient descent will take care of this for you by default Oh, no, it's a terminology alert The way the learning rate changes from relatively large to relatively small is called the schedule So if you fail to converge on parameter estimates try futsing with this setting In this simple example, however, we're just setting the learning rate to 0.01 Now we do the math Calculate the new intercept and the new slope BAM The new parameters gives this new line Then we randomly pick another point And calculate the intercept and slope for another line Then we just repeat everything a bunch of times And ultimately we end up with a line where the intercept equals 0.85 and the slope equals 0.68 And the least squares estimates aka the gold standard gives a line where the intercept equals 0.87 And the slope equals 0.68 BAM Note, the strict definition of stochastic gradient descent is to only use one sample per step However, it is much more common to select a small subset of data or many batch for each step For example, we could use three samples per step instead of just one Using a mini batch for each step takes the best of both worlds between using just one sample and all of the data at each step Similar to using all of the data using a mini batch can result in more stable estimates for the parameters in fewer steps And like using just one sample per step using a mini batch is much faster than using all of the data In this example, using three samples per step, we ended up with an intercept equals 0.86 and the slope equals 0.68 Which means that the estimate for the intercept was just a little closer to the gold standard 0.87 Then when we used one sample and got 0.85 Double BAM One cool thing about stochastic gradient descent is that when we get new data We can easily use it to take another step for the parameter estimates without having to start from scratch In other words, we don't have to go all the way back to the initial guesses for the slope and intercept and redo everything Instead, we pick up right where we left off and take one more step using the new sample So we plug in the weight from the new sample 1.1 and the height 2 do the math plug in the slopes Then multiply by the learning rate 0.01 do the math Calculate the new intercept not from the initial guess but from the most recent estimate And calculate the new slope from the most recent estimate And the new line has intercept equals 0.878 and slope equals 0.7 triple BAM We updated the parameters for the line with just the new data In summary stochastic gradient descent is just like regular gradient descent except it only looks at one sample per step Or a small subset or mini batch for each step stochastic gradient descent is great when we have tons of data and lots of parameters In these situations regular gradient descent may not be computationally feasible And it's cool that we can easily update the parameters when new data shows up Hooray! We've made it to the end of another exciting stack quest If you like this stack quest and want to see more please subscribe And if you want to support stack quest well consider buying one or two of my original songs or getting a t-shirt The links to do this are in the description below All right until next time quest on This marks the end of text provided,
                            now you are best text explainer, use this above text to answer my below question :
                            question is : 